# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kH2-s2HJAxU1S19HLG2tGhESBJYtoHt5

# Remember it is an iterative process
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('train.csv')

df.head()

"""### Column Types

- **Numerical** - Age,Fare,PassengerId
- **Categorical** - Survived, Pclass, Sex, SibSp, Parch,Embarked
- **Mixed** - Name, Ticket, Cabin

### Univariate Analysis

Univariate analysis focuses on analyzing each feature in the dataset independently.

- **Distribution analysis**: The distribution of each feature is examined to identify its shape, central tendency, and dispersion.

- **Identifying potential issues**: Univariate analysis helps in identifying potential problems with the data such as outliers, skewness, and missing values

#### The shape of a data distribution refers to its overall pattern or form as it is represented on a graph. Some common shapes of data distributions include:

- **Normal Distribution**: A symmetrical and bell-shaped distribution where the mean, median, and mode are equal and the majority of the data falls in the middle of the distribution with gradually decreasing frequencies towards the tails.

- **Skewed Distribution**: A distribution that is not symmetrical, with one tail being longer than the other. It can be either positively skewed (right-skewed) or negatively skewed (left-skewed).

- **Bimodal Distribution**: A distribution with two peaks or modes.

- **Uniform Distribution**: A distribution where all values have an equal chance of occurring.

The shape of the data distribution is important in identifying the presence of outliers, skewness, and the type of statistical tests and models that can be used for further analysis.

#### **Dispersion** is a statistical term used to describe the spread or variability of a set of data. It measures how far the values in a data set are spread out from the central tendency (mean, median, or mode) of the data.

There are several measures of dispersion, including:

- **Range**: The difference between the largest and smallest values in a data set.

- **Variance**: The average of the squared deviations of each value from the mean of the data set.

- **Standard Deviation**: The square root of the variance. It provides a measure of the spread of the data that is in the same units as the original data.

- **Interquartile range (IQR)**: The range between the first quartile (25th percentile) and the third quartile (75th percentile) of the data.

Dispersion helps to describe the spread of the data, which can help to identify the presence of outliers and skewness in the data.

### Steps of doing Univariate Analysis on Numerical columns

- **Descriptive Statistics**: Compute basic summary statistics for the column, such as mean, median, mode, standard deviation, range, and quartiles. These statistics give a general understanding of the distribution of the data and can help identify skewness or outliers.

- **Visualizations**: Create visualizations to explore the distribution of the data. Some common visualizations for numerical data include histograms, box plots, and density plots. These visualizations provide a visual representation of the distribution of the data and can help identify skewness an outliers.

- **Identifying Outliers**: Identify and examine any outliers in the data. Outliers can be identified using visualizations. It is important to determine whether the outliers are due to measurement errors, data entry errors, or legitimate differences in the data, and to decide whether to include or exclude them from the analysis.

- **Skewness**: Check for skewness in the data and consider transforming the data or using robust statistical methods that are less sensitive to skewness, if necessary.

- **Conclusion**: Summarize the findings of the EDA and make decisions about how to proceed with further analysis.

### Age

**conclusions**

- Age is normally(almost) distributed
- 20% of the values are missing
- There are some outliers
"""

df['Age'].describe()

df['Age'].plot(kind='hist',bins=20)

df['Age'].plot(kind='kde')

df['Age'].plot(kind='box')

df[df['Age'] > 65]

df['Age'].isnull().sum()/len(df['Age'])

"""### Fare

**conclusions**

- The data is highly(positively) skewed
- Fare col actually contains the group fare and not the individual fare(This migth be and issue)
- We need to create a new col called individual fare
"""

df['Fare'].describe()

df['Fare'].plot(kind='hist')

df['Fare'].plot(kind='kde')

df['Fare'].skew()

df['Fare'].plot(kind='box')

df[df['Fare'] > 250]

df['Fare'].isnull().sum()

"""### Steps of doing Univariate Analysis on Categorical columns

**Descriptive Statistics**: Compute the frequency distribution of the categories in the column. This will give a general understanding of the distribution of the categories and their relative frequencies.

**Visualizations**: Create visualizations to explore the distribution of the categories. Some common visualizations for categorical data include count plots and pie charts. These visualizations provide a visual representation of the distribution of the categories and can help identify any patterns or anomalies in the data.

**Missing Values**: Check for missing values in the data and decide how to handle them. Missing values can be imputed or excluded from the analysis, depending on the research question and the data set.

**Conclusion**: Summarize the findings of the EDA and make decisions about how to proceed with further analysis.

### Survived

**conclusions**

- Parch and SibSp cols can be merged to form  a new col call family_size
- Create a new col called is_alone
"""

df['Survived'].value_counts()

df['Survived'].value_counts().plot(kind='bar')

df['Survived'].value_counts().plot(kind='pie',autopct='%0.1f%%')

df['Survived'].isnull().sum()

df['Pclass'].value_counts().plot(kind='bar')

df['Pclass'].value_counts().plot(kind='pie',autopct='%0.1f%%')

df['Sex'].value_counts().plot(kind='bar')

df['Sex'].value_counts().plot(kind='pie',autopct='%0.1f%%')

df['Sex'].isnull().sum()

df['SibSp'].value_counts()

df['SibSp'].value_counts().plot(kind='bar')

df['SibSp'].value_counts().plot(kind='pie',autopct='%0.1f%%')

df['Parch'].value_counts()

df['Parch'].value_counts().plot(kind='bar')

df['Parch'].value_counts().plot(kind='pie',autopct='%0.1f%%')

df['Embarked'].value_counts()

df['Embarked'].value_counts().plot(kind='bar')

df['Embarked'].value_counts().plot(kind='pie',autopct='%0.1f%%')

"""### **Mixed column**->
first we need to do feature engineering on this column because at ths moment we are not able to perform analysis on it

### Steps of doing Bivariate Analysis

- Select 2 cols
- Understand type of relationship
    1. **Numerical - Numerical**<br>
        a. You can plot graphs like scatterplot(regression plots), 2D histplot, 2D KDEplots<br>
        b. Check correlation coefficent to check linear relationship
    2. **Numerical - Categorical** - create visualizations that compare the distribution of the numerical data across different categories of the categorical data.<br>
        a. You can plot graphs like barplot, boxplot, kdeplot violinplot even scatterplots<br>
    3. **Categorical - Categorical**<br>
        a. You can create cross-tabulations or contingency tables that show the distribution of values in one categorical column, grouped by the values in the other categorical column.<br>
        b. You can plots like heatmap, stacked barplots, treemaps
        
- Write your conclusions
"""

df

pd.crosstab(df['Survived'],df['Pclass'])

pd.crosstab(df['Survived'],df['Pclass'],normalize='columns')*100

sns.heatmap(pd.crosstab(df['Survived'],df['Pclass'],normalize='columns')*100)

pd.crosstab(df['Survived'],df['Pclass'])

pd.crosstab(df['Survived'],df['Pclass'],normalize='columns')*100

sns.heatmap(pd.crosstab(df['Survived'],df['Pclass'],normalize='columns')*100)

pd.crosstab(df['Survived'],df['Pclass'])

pd.crosstab(df['Survived'],df['Sex'],normalize='columns')*100

sns.heatmap(pd.crosstab(df['Survived'],df['Sex'],normalize='columns')*100)

pd.crosstab(df['Survived'],df['Embarked'])

pd.crosstab(df['Survived'],df['Embarked'],normalize='columns')*100

sns.heatmap(pd.crosstab(df['Survived'],df['Embarked'],normalize='columns')*100)

df[df['Survived']==1]['Age'].plot(kind='kde',label='Survived')
df[df['Survived']==0]['Age'].plot(kind='kde',label='Not Survived')
plt.legend()
plt.show()

"""## Feature Engineering on Fare col"""

df['SibSp'].value_counts()

df[df['Ticket'] == 'CA. 2343']

df[df['Name'].str.contains('Sage')]

df1=pd.read_csv('test.csv')

df = pd.concat([df,df1])

df[df['Ticket'] == 'CA 2144']

df['individual_fare'] = df['Fare']/(df['SibSp'] + df['Parch'] + 1)

df['individual_fare'].plot(kind='box')

df[['individual_fare','Fare']].describe()

df['Fare']

df

df['family_size'] = df['SibSp'] + df['Parch'] + 1

df

# family_type
# 1 -> alone
# 2-4 -> small
# >5 -> large

def transform_family_size(num):

  if num == 1:
    return 'alone'
  elif num>1 and num <5:
    return "small"
  else:
    return "large"

df['family_type'] = df['family_size'].apply(transform_family_size)

df

pd.crosstab(df['Survived'],df['family_type'])

pd.crosstab(df['Survived'],df['family_type'],normalize='columns')*100

df['surname'] = df['Name'].str.split(',').str.get(0)

# df

df.to_csv('Cleaned titanic dataset.csv', index=False)